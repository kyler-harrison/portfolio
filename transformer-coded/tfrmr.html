<!DOCTYPE html>
<html>
	<head>
		<title>Transformers - Part 2: Training models</title>
		<link href="../page.css" rel="stylesheet" type="text/css">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    	</script>
	  <script type="text/javascript"
       src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
	</head>
	<body>
		<ul class="nav-bar">
			<li class="nav-li nav-li-right"><a class="nav-a" href="../index.html">Kyler Harrison</a></li>
		</ul>
		<div class="content">
			<div class="title-container">
				<h1>Transformers - Part 2: Training models</h1>
				<h2>Creating a crappy Google translate</h2>
				<p class="header-text">10/13/2023</p>
				<hr>
			</div>
			<div class="main-text-container">
				<!--<h2>Is it intelligence or just a clever sequence of matmuls?</h2>-->
				<p>In a previous <a href="../transformer/transformer.html" target="_blank">post</a>, I detailed my comprehension of the paper "Attention is all you need" (<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">arXiv link</a>). Using the transformer code presented in that post, this post describes the code how I trained a transformer to translate english sentences to spanish sentences.</p>
				<p>For code specific to discussion points, I've created Github gists and have embedded their displays in this webpage. The full repo of code that I actually worked in can be found <a target=_blank href="https://github.com/kyler-harrison/transformer-en-es/tree/main">here</a>.</p>
				<br>
				<h2>Data collection and preparation</h2>
				<p>The most important part of any data science/machine learning project is the data. <b>Always remember: a good model will never fix bad data</b>. Considering this, I'll admit that I wasn't very thorough when I selected my dataset for this project. I grabbed the first english-spanish sentence pair dataset I found and decided it wasn't such a big deal if I understood the data deeply since I just wanted to see if I could get a transformer to learn something. If this was a real task and people were depending on the results, I would have spent the majority of my time focused on understanding the dataset instead of toying around with different model parameters. Anyways, the dataset I used can be found <a href="https://www.kaggle.com/datasets/lonnieqin/englishspanish-translation-dataset?resource=download" target=_blank>here</a>.</p>
				<p>I decided to use pre-trained token embeddings for the english and spanish tokens. I did this partly because I didn't understand embedding layers when I started coding the transformer, and partly because I figured pre-trained embeddings would make my transformer learn quicker. In retrospect, I'm not sure if that assumption was valid and I wish I had written my code in a more modular fashion so that I could test out how the transformer performs when it learns the embeddings itself. I found word2vec models <a href="https://fasttext.cc/docs/en/crawl-vectors.html" target=_blank>here</a> and opted for the text file download containing the direct mapping between words and their vectors.</p>
				<p>I changed the formatting of the downloaded word2vec dictionaries with a <a href="https://github.com/kyler-harrison/transformer-en-es/blob/main/data_utils.py" target=_blank>small script</a>. This script makes a simple csv file in the format of "word,vector" and this format is assumed later when data is loaded.</p>
				<p>Next, I reduced the size of the word2vec dictionaries. The downloaded dictionaries were several GB in size and contained tokens that were not found in the dataset. <a href="https://github.com/kyler-harrison/transformer-en-es/blob/main/reduce_data.ipynb" target=_blank >This notebook</a> removes all tokens from the word2vec dictionaries that are not present in the dataset and removes all rows from the dataset that don't contain words in the word2vec dictionaries. I don't feel the need to go through the details of that implementation here. There were 111,184 rows in the dataset remaining after this reduction.</p>
				<p>I wrote several functions to transform the raw string data into embedded and positionally encoded input matrices. Most of this code is found in <a href="https://github.com/kyler-harrison/transformer-en-es/blob/main/preprocess.py" target=_blank>preprocess.py</a>. The logic and reasoning for each of these functions is explained in the previous writeup.</p>
				<p>I decided to drop all rows of the dataset that contained more than 13 tokens in english and 14 tokens in spanish (I did 1 more token for spanish to account for double punctuation). These numbers were decided somewhat arbitrarily based on the distribution of number of tokens. The resulting dataset has 97,443 rows and an approximately normal distribution of number of tokens.</p>
				<div class="img-container">
					<img src="en_num_tokens.png">
					<img src="es_num_tokens.png">
				</div>
				<br>
				<h2>Model experiments</h2>
				<p>I trained several different models with varying parameters. Each model was trained for 20 epochs. Each training epoch consisted of passing the entire training dataset through the transformer in batches, and then performing inference on the entire testing dataset in batches. A single epoch took about 25 minutes on my PC (i7-12700K CPU with 3.60 GHz base clock, 12 cores, 20 total threads; NVIDIA GeForce RTX 3060 with 12 GB GDDR6). So, it took a little over 8 hours to train each model. Conveniently, I sleep about that much every night (and I don't use my PC to sleep).</p>
				<p><i>A quick side note:</p> 
				<p>According to a few ~reliable leaks (<a href="https://towardsdatascience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae#:~:text=According%20to%20unverified%20information%20leaks,8%20=%203%2C125%20servers%20were%20needed." target=_blank>here</a> and <a href="https://www.reddit.com/r/OpenAI/comments/11sbw40/estimate_of_amount_of_compute_used_to_train_gpt4/" target=_blank>here</a> and other places that I've seen), GPT-4 was trained on over 10,000 (maybe 25,000) NVIDIA A100s for 3-4 months. I can't seem to find a concrete price tag for the 80 GB version of this graphics card and it looks like you have to manually contact NVIDIA sales to get a consultation of sorts (i.e. I probably don't qualify to own one and definitely can't afford one), but it looks like most estimates are in the range of <i>$</i>8,000-<i>$</i>15,000. So, let's just call it $10,000 per card. That gives a total GPU cost in the neighborhood of <i>$</i>100e6-<i>$</i>250e6. Now, "Open"AI has some sort of partnership with Microsoft, so they probably got a sweet discount, but still. Do you have <i>$</i>100e6 to buy GPUs with? And an additional <i>$</i>50e6 to employ 500 skilled people? And idk, probably another <i>$</i>50e6 for the physical space required to house the GPUs, employees, and to cover all of the intermediate expenses involved in doing so? Oh, and don't forget that power bill. So, who gets to control the future of AI (and by philosophical extension, humanity)? Hint: it's not "we the people." Realistically, there are only a handful of organizations/corporations/entities on this planet that control enough capital to even afford the current state-of-the-art AI. Hmmm, really makes you wonder, who does AI regulation really protect?</p>
				<p>"Our mission is to ensure that artificial general intelligence benefits all of humanity." The AGI hype is fear-mongering, a distraction from the reality: an opulent minority wants to monopolize a cool new technology and make everyone on the planet dependent upon it. Nothing new here.</i></p>
				<p>Anyways, I evaluated the loss on the training and testing datasets and calculated the <a href="https://aclanthology.org/P02-1040.pdf" target=_blank>1-gram BLEU score</a> after each epoch for each model trained. The parameters changed across models were the batch size, number of heads in the transformer (h), number of encoder/decoder layers in the transformer (N), train/test split ratio, model hyperparameters, and dropout + label smoothing. In the series of graphics below, each model is an iteration on the previous model displayed (i.e. I changed one parameter at a time and then retrained to see performance differences).</p>
				<p>The first model fit used a 90/10 train/test split, h=1, N=1, batch size of 128, default Adam optimizer parameters (as defined by PyTorch), and no dropout + label smoothing.</p>
				<div class="img-container">
					<img width=650px src="v0_N=1_h=1.png">
				</div>
				<p>The next model used the same parameters as the previous, but with h=4 and N=4.</p>
				<div class="img-container">
					<img width=650px src="v0_N=4_h=4.png">
				</div>
				<p>As N and h increased, I quickly began to run into the memory limits of my graphics card, so I decided to just stick with N=1 and h=1 for the rest of the modeling experiments.</p>
				<p>The next model used an 80/20 train/test split. I was curious to see if the model would overfit less. It seemed to just take longer to converge to the previous train/test loss values obtained as the 90/10 split models.</p>
				<div class="img-container">
					<img width=650px src="v1_N=1_h=1.png">
				</div>
				<p>Next, I changed the Adam optimzation parameters to those provided in the paper. The parameters listed in the paper are smaller than those used by default in PyTorch. As expected, this led to a slower convergence to previous train/test loss values obtained. The 1-gram BLEU score also obtained a new high around 0.19 (to be fair, this was barely higher than the previous max of ~0.184).</p>
				<div class="img-container">
					<img width=650px src="v2_N=1_h=1.png">
				</div>
				<p>Next, I increased the batch size to 256. I hoped that this would lead to less overfitting. It appeared to decrease the difference between train and test loss a little bit while attaining a similar 1-gram BLEU score.</p>
				<div class="img-container">
					<img width=650px src="v3_N=1_h=1.png">
				</div>
				<p>The last iteration included dropout + label smoothing (used in the paper). I expected this to be the best model fit because I figured these changes would lead to better generalizability, but the results didn't suggest improvement over the previous model. I was especially confused by the early 1-gram BLEU score of ~0.19 followed by a dropoff and an expected increase as seen in previous models.</p>
				<div class="img-container">
					<img width=650px src="v4_N=1_h=1.png">
				</div>
				<br>
				<h2>Discussion of results and next steps</h2>
				<p>Somewhat subjectively, somewhat quantitatively, I decided that the best model fit was the one that used an 80/20 train/test split, a batch size of 256, h=1, N=1, and the Adam parameters described in the paper (second-to-last metrics graph above). I wrote a simple <a href="https://github.com/kyler-harrison/transformer-en-es/blob/main/repl.py" target=_blank>REPL</a> to get english-spanish translations on the fly.</p>
				<p>Here's an example of some perfect (when compared to Google translate outputs) translations:</p>
				<div class="img-container">
					<img width=500px src="perfect_examples.png">
				</div>
				<p>Here are a few translations that contain errors:</p>
				<div class="img-container">
					<img width=550px src="error_examples.png">
				</div>
				<p>If you don't speak any spanish, the errors in the above example may not be obvious to you. When translated back to english with Google translate, these sentences are:</p>
				<div class="img-container">
					<img width=550px src="tall.png">
				</div>
				<div class="img-container">
					<img width=550px src="first_first.png">
				</div>
				<div class="img-container">
					<img width=550px src="size.png">
				</div>
				<p>Of course, I can't enumerate every possible translation in the space of the model's possible inputs, but I've tried ~100 translations and have noticed a few general behaviors (these are mostly qualitative observations):</p>
				<ul>
					<li>translations are more likely to be perfect when the input english sentence is a short question or command that contains simple words</li>
					<li>translations are often nearly correct but will contain duplicate words or phrases (see above, for example)</li>
					<li>translations are often nearly correct but will contain incorrect verb conjugation (I'm led to believe that without more context, this is a difficult problem to solve in any translation task)</li>
					<li>punctuation that does not end a sentence is usually ignored (commas may be an exception)</li>
					<li>if punctuation is missing from the input, it will be appropriately added to the end of the output (works with question marks too)</li>
					<li>inputs containing repeated words are handled pretty well, for example "ball ball ball ball ball ball ball ball ball ball ball" will translate to "pelota pelota pelota pelota pelota pelota pelota pelota pelota pelota pelota"</li>
					<li>erroneous translations will sometimes contain words and/or phrases that are conceptually similar to the true concept (this might be the most interesting observation as it displays the power of the attention mechanism)</li>
				</ul>
				<br>
				<h2>The end (I hate LLMs)</h2>
				<p>At this point, all I want to say is that this project was a ton of work and language ML tasks are complicated and I don't want to discuss LLMs anymore.</p>
			</div>
		</div>
	</body>
</html>
